---
date: "2020-09-11"
featureImage: images/allpost/homework2.png
postImage: images/single-blog/0.jpg
title: Homework 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(kableExtra)
library(infer)
```


# Climate change and temperature anomalies 

We want to study climate change and gratefully find data on the *Combined Land-Surface Air and Sea-Surface Water Temperature Anomalies* in the Northern Hemisphere at [NASA's Goddard Institute for Space Studies](https://data.giss.nasa.gov/gistemp). The [tabular data of temperature anomalies can be found here](https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.txt).

To define temperature anomalies, we need to have a reference, or base period, which NASA clearly states that it is the period between 1951-1980.

Let's load the data and have a look!

```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.csv", 
           skip = 1, # start in row 2
           na = "***") # recognize *** as NAs

```

For each month and year, the dataframe shows the deviation of temperature from the normal (expected). Furthermore, the dataframe is in wide format. Hence, we first bring it into long format, which makes it easier to analyse the data:

```{r tidyweather}

# bring data in tidy format
tidyweather <- weather %>%
  # only use year and month columns
  select(Year:Dec) %>%
  # bring into long format
  pivot_longer(cols = Jan:Dec, names_to = "Month", values_to = "delta") 

# print 10 rows of the table to have a look at the transformed data
tidyweather %>% 
  head(10) %>% 
  kbl	() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

```{r glimpse_tidyweather}

glimpse(tidyweather)

```

Note that `Year` and `Month` are stored as `double` and `character` variables (and not as `date` variables).


## Plotting Information

To be able to create a nice time series plot, we first need to create a new variable called `date`, in order to ensure that the `delta` values are plot chronologically. 

```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), Month, "1")), # create `date` variable with lubridate
         month = month(date, label=TRUE),
         year = year(date))

# plot a timeseries scatterplot of all the deltas
ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point() +
  geom_smooth(color="red") + # add smoothed red line
  theme_bw() +
  labs (
    title = "Increasing Weather Anomalies Demonstrating Global Warming",
    caption = "Source: NASA"
  )

```

As we can clearly see, there seems to be a strong trend towards warm weather anomalies!

Is the effect of increasing temperature more pronounced in some months? Let's have a look: 

```{r facet_wrap, echo=FALSE}

# plot weather anomalies faceted by month
ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point() +
  facet_wrap(~month) + # facet by month (the lubridate month!)
  geom_smooth(color="red") + # add red smoothed line
  theme_bw() +
  labs (
    title = "Increasing Weather Anomalies for All Months",
    caption = "Source: NASA"
  )

```

We can see that the effect is almost the same for every month!

As a next step for our analysis of global warming, we group the years into bigger periods:

```{r intervals}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```

Using our new `interval` variable, we are able to create a density plot to study the distribution of monthly deviations, grouped by the different time periods.

```{r density_plot}

ggplot(comparison, aes(x=delta, fill=interval))+
  geom_density(alpha=0.2) +   # density plot with transparency set to 20%
  theme_bw() +                # theme
  labs (
    title = "The Bells are Moving!",
    subtitle = "Density Plot for Monthly Temperature Anomalies",
    y     = "Density",         # changing y-axis label to sentence case
    caption = "Source: NASA"
  )

```

Again, we can clearly recognize the pattern of global warming!

So far, we have been working with monthly anomalies. However, we are also interested in average annual anomalies.

Let's have a look, how they look like:

```{r averaging}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(annual_average_delta = mean(delta, na.rm=TRUE)) 

#plotting the data:
ggplot(average_annual_anomaly, aes(x=year, y= annual_average_delta))+
  geom_point()+
  #Fit the best fit line, using LOESS method
  geom_smooth(method = "loess") +
  #change to theme_bw() to have white background + black frame around plot
  theme_bw() +
  labs (
    title = "Global Warming is REAL!!",
    subtitle = "Average Yearly Anomaly",
    y     = "Average Annual Delta",
    caption = "Source: NASA"
  )                         


```

Once more, the message is pretty clear: global warming is real! 


## Confidence Interval for `delta`

[NASA points out on their website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php) that 

> A one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age.

So, do we already deal with a one-degree global change? For this purpose, we calculate a confidence interval for the time between 2011-present.

First, we want to do this using traditional formulas:

```{r calculate_CI_using_formula}

formula_ci <- comparison %>% 
  # choose the interval 2011-present
  filter(interval == "2011-present", !is.na(delta)) %>%
  # calculate summary statistics for temperature deviation (delta) 
  # calculate mean, SD, count, SE, margin of error and lower/upper 95% CI
  summarise(mean_delta = mean(delta),
            sd_delta = sd(delta),
            count = n(),
            # get t-critical value with (n-1) degrees of freedom
            t_critical = qt(0.975, count-1),
            se_delta = sd_delta/sqrt(count),
            margin_of_error = t_critical * se_delta,
            delta_low = mean_delta - margin_of_error,
            delta_high = mean_delta + margin_of_error
  ) 

#print out formula_CI
formula_ci %>% 
  select(delta_low, mean_delta, delta_high) %>% # only show relevant columns
  rename(`lower bound` = delta_low, `mean delta` = mean_delta, `upper bound` = delta_high) %>% # create readable names
  kbl(caption = "confidence interval for mean delta since 2011 (formula)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

```
Hence, we get an average delta of 0.966 and a confidence interval of [0.915, 1.02] under 97.5% confidence. As we can see, it seems not unlikely that there already is a one-degree global warming!

However, some people trust more into the simulation skills of their computers than into mathematical formulas. Hence, let's calculate the confidence interval using bootstrapping!

```{r calculate_CI_using_bootstrap}

# use the infer package to construct a 95% CI for delta

set.seed(1234)

boot_weather <- comparison %>%
  # Choose only 2011-present period
  filter(interval == "2011-present", !is.na(delta)) %>%
  # Specify the variable of interest
  specify(response = delta) %>%
  # Generate a bunch of bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>%
  # Find the mean of each sample
  calculate(stat = "mean")

# calculate confidence interval
percentile_ci <- boot_weather %>%
  get_ci(level=0.95, type="percentile")

# print out confidence interval
percentile_ci %>% 
  rename(`lower bound` = lower_ci, `upper bound` = upper_ci) %>% # create readable names
  kbl(caption = "confidence interval for mean delta since 2011 (simulated)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

```

```{r visualise_ci_bootstrap}
# visualise both confidence intervals and the bootstrap distribution
ggplot(boot_weather, aes(x = stat)) +
  geom_histogram() +
  labs(title= "Bootstrap distribution of means with confidence intervals") +
  # add confidence interval bounds as vertical lines
  geom_vline(aes(xintercept = percentile_ci$lower_ci, colour = 'bootstrap CI'), linetype = 2, size = 1.1) +
  geom_vline(aes(xintercept = percentile_ci$upper_ci, colour = 'bootstrap CI'), linetype = 2, size = 1.1) +
  geom_vline(aes(xintercept = formula_ci$delta_low, colour = 'formula CI'), linetype = 1, size = 1.1) +
  geom_vline(aes(xintercept = formula_ci$delta_high, colour = 'formula CI'), linetype = 1, size = 1.1) +
  # change color of the lines
  scale_color_manual(name = NULL, values = c(`bootstrap CI` = "red", `formula CI` = "orange")) +
  theme_bw()

```

We can see that the simulated CI and the mathematically calculated CI are very similar.  

For the simulated CI, we resampled our sample 1000 times with replication (--> bootstrapping). 

For each of these 1000 "new" samples, we then calculated the average delta and obtained the bootstrap distribution as depicted above.  

With the help of this simulated / approximated sample distribution (distribution of the mean delta), we were able to create our simulated CI by choosing upper and lower bounds, such that 95% of the (resampled) means are covered.


# General Social Survey (GSS)

The [General Social Survey (GSS)](http://www.gss.norc.org/) gathers data on American society in order to monitor and explain trends in attitudes, behaviours, and attributes. Many trends have been tracked for decades, so one can see the evolution of attitudes, etc in American Society.

Let's load this intersting data for the year 2016:

```{r read_gss_data, cache=TRUE}
gss <- read_csv(here::here("project_data", "smallgss2016.csv"), 
                na = c("", "Don't know",
                       "No answer", "Not applicable", "NA"))
```

The variables we have in our data are the following:

- hours and minutes spent on email weekly. The responses to these questions are recorded in the `emailhr` and `emailmin` variables.
- `snapchat`, `instagrm`, `twitter`: whether respondents used these social media (in 2016)
- `sex`: Female or Male
- `degree`: highest education level attained

## Instagram and Snapchat, by sex

We will now estimate the *population* proportion of Snapchat or Instagram users in 2016:

```{r snap_insta_df}

# calculate snap_insta, that measures Instagram and Snapchat use in combination
snap_insta_df <- gss %>%
  # "Yes" if the respondent reported using any of snapchat or instagrm, and "No" if not. If both NA then also NA.
  mutate(snap_insta = case_when(snapchat == "Yes" | instagrm == "Yes" ~ "Yes",
                                is.na(snapchat) & is.na(instagrm) ~ NA_character_ ,
                                TRUE ~ "No")
         )

# print overall proportion
snap_insta_df %>%
  summarise(`number of instagram/ snapchat users` = count(snap_insta == "Yes"),
            `number of people who use neither` = count(snap_insta == "No"),
            `proportion of instagram/ snapchat users` = count(snap_insta == "Yes")/count(snap_insta %in% c("Yes","No"))) %>%
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

```

Hence, our best guess is that overall 37.5% of the whole population use instagram, snapchat, or both.

But it's always better to not always look at point estimates, but at confidence intervals. As we are also interested in the difference between men and women, we calculate two CIs - on for men, one for women - for the snapchat / instagram proportion with traditional mathematical formulas:

```{r snap_insta_CI}

# calculate CIs for men and women
sex_formula_ci <- snap_insta_df %>%
  filter(!is.na(snap_insta)) %>% 
  group_by(sex) %>%  # group by sex and calculate CIs
  summarise(`proportion of instagram/ snapchat users` = count(snap_insta == "Yes")/count(snap_insta %in% c("Yes","No")),
            z_critical = qnorm(0.975),
            count = n(),
            # Confidence Interval for proportion = p  +/-  z*(√p(1-p) / n)
            se_proportion = sqrt((`proportion of instagram/ snapchat users`*(1-`proportion of instagram/ snapchat users`))/count),
            margin_of_error = z_critical * se_proportion,
            `lower bound` = `proportion of instagram/ snapchat users` - margin_of_error,
            `upper bound` = `proportion of instagram/ snapchat users` + margin_of_error
  )


# print table with CIs
sex_formula_ci %>%
  select(sex, `lower bound`, `proportion of instagram/ snapchat users`, `upper bound`) %>% # only show relevant columns
  kbl(caption = "confidence interval on proportion of instagram/snapchat users by sex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

# plot CIs
ggplot(sex_formula_ci, aes(y=sex, x = `proportion of instagram/ snapchat users`, group = sex, color=sex)) +
  #draws the means
  geom_point(size = 5) +
  #draws the CI error bars
  geom_errorbar(aes(xmin=`lower bound`, xmax=`upper bound`), width=.1, size = 2) +
  theme_bw() +
  theme(legend.position = "none")+
  labs(title = "Women Spend More Hours on Social Media",
       subtitle = "Confidence Intervals of Instagram/Snapchat Usage")

```

We can clearly see that the data supports the view that many more women use instagram and snapchat than men do!

## Twitter, by education level

Wait, there is a another social media, for which we have data! Let's have a look at Twitter! What do you think bachelor and graduate students' usage of Twitter would be?

To find out, we have to clean the data first. 

First, let's turn `degree` from a character variable into a factor variable, in ascending order of years of education (Lt high school, High School, Junior college, Bachelor, Graduate). 

We then create a new variable, `bachelor_graduate`, that is *Yes* if the respondent has either a `Bachelor` or `Graduate` degree, in order to distinguish bachelor/graduate students from others.


```{r gss_modify}

# #inspect distinct value of degree
# gss %>%
#   select(degree) %>%
#   distinct(degree)

gss_modify <- gss %>%
  # make degree a factor with the right ordering
  mutate(degree = factor(degree,level=c("Lt high school",
                                        "High school",
                                        "Junior college",
                                        "Bachelor",
                                        "Graduate")),
         # create bachelor_graduate variable
         bachelor_graduate = case_when(degree %in% c("Bachelor","Graduate") ~ "Yes",
                                       is.na(degree) ~ NA_character_,
                                       TRUE ~ "No")
         )

```

Third, let's calculate the overall proportion of Twitter users:

```{r bachelor_graduate_prop_2}

gss_modify %>% 
  # drop observations that miss information
  drop_na(c(bachelor_graduate,twitter)) %>% 
  # calculate overall Twitter proportion
  summarise(`people who use Twitter` = count(twitter == "Yes"),
            `people who don't use Twitter` = count(twitter == "No"),
            `proportion of those who use Twitter` = `people who use Twitter` / (`people who use Twitter` + `people who don't use Twitter`),
            `proportion of those who don't use Twitter` = 1 - `proportion of those who use Twitter`) %>%
  kbl(caption = "Overall Proportion of Twitter users") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

```
Let's now bring in our newly created variable `bachelor_graduate` and see how Twitter usage varies between those with a degree vs those without a degree:


```{r bachelor_graduate_prop}

gss_modify %>% 
  # drop observations that miss information
  drop_na(c(bachelor_graduate,twitter)) %>% 
  group_by(bachelor_graduate) %>% # group by bachelor_graduate and calculate proportions of Twitter users
  rename(`have a higher educational degree?` = bachelor_graduate) %>% # have a readable name
  summarise(`people who use Twitter` = count(twitter == "Yes"),
            `people who don't use Twitter` = count(twitter == "No"),
            `proportion of those who use Twitter` = `people who use Twitter` / (`people who use Twitter` + `people who don't use Twitter`),
            `proportion of those who don't use Twitter` = 1 - `proportion of those who use Twitter`) %>%
  kbl(caption = "Overall Proportion of Twitter users by education level") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

```

We see that students with a degree use Twitter more. But how about the confidence intervals?

Using the CI formula for proportions, let's construct two 95% CIs for `bachelor_graduate` vs whether they use (Yes) and don't (No) use twitter. 

```{r bachelor_graduate_ci}

bachelor_graduate_formula_ci <- gss_modify %>%
  # drop observations that miss information
  drop_na(c(bachelor_graduate, twitter)) %>% 
  group_by(bachelor_graduate) %>% # group by bachelor_graduate and calculate CIs
  # Confidence Interval = p  +/-  z*(√p(1-p) / n)
  summarise(`proportion of twitter users` = count(twitter == "Yes") / count(twitter %in% c("Yes","No")),
            z_critical = qnorm(0.975),
            count = n(),
            se_proportion = sqrt((`proportion of twitter users`*(1-`proportion of twitter users`))/count),
            margin_of_error = z_critical * se_proportion,
            `lower CI bound` = `proportion of twitter users` - margin_of_error,
            `upper CI bound` = `proportion of twitter users` + margin_of_error
  )

# print out CIs
bachelor_graduate_formula_ci %>%
  select(bachelor_graduate, `lower CI bound`, `proportion of twitter users`, `upper CI bound`) %>% # only show relevant columns
  rename(`have a higher educational degree?` = bachelor_graduate) %>% # have readable names
  kbl(caption = "Confidence Intervals for Twitter usage") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

# plot CIs
ggplot(bachelor_graduate_formula_ci, aes(y=bachelor_graduate, 
                                         x = `proportion of twitter users`, 
                                         group = bachelor_graduate, 
                                         color=bachelor_graduate)) +
  #draws the means
  geom_point(size = 5) +
  #draws the CI error bars
  geom_errorbar(aes(xmin=`lower CI bound`, xmax=`upper CI bound`), width=.1, size = 2) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(y = "have a higher educational degree?",
       x = "Proportion of Twitter users",
       title = "People with a higher education degree use Twitter more!")

```
The confidence interval for the proportion of Twitter usage is [13.6%, 18.4%] for those who do not have a degree, vs [19.6%, 27.1%] for thos who have a degree. They do not overlap.

Obviously, more bachelor/graduate student are using Twitter! Are you the majority?

## Email usage

Finally, we also want to have a look at the time people spend on email. How many minutes the respondents spend on email weekly?

```{r email_time}

gss <- gss %>% 
  # calculate new variable `email` (notice that either both or non of `emailhr` and `emailmin` are NA!)
  mutate(email = 60*emailhr + emailmin)

email_summary_statistics <- gss %>% 
  summarise(mean = mean(email, na.rm = TRUE), median = median(email, na.rm = TRUE))

# visualise distribution and mean and median
gss %>% 
  ggplot(aes(x = email)) +
  geom_histogram(binwidth = 30) + # binwidth is 30 min
  theme_bw() +
  # add median and mean lines
  geom_vline(aes(xintercept = email_summary_statistics$median, colour = 'median'), linetype = 2, size = 1) +
  geom_vline(aes(xintercept = email_summary_statistics$mean, colour = 'mean'), linetype = 2, size = 1) +
  # change color of lines
  scale_color_manual(name = NULL, values = c(median = "red", mean = "blue")) +
  # show median in legend before mean
  guides(color = guide_legend(reverse = TRUE)) +
  labs(x = "email usage in minutes per week",
       title = "Distribution of Email Usage")

```

As shown in the graph, we deal with a right skewed (unbalanced, lopsided) distribution, where the mean is farther out in the long tail than is the median.

Therefore the median is usually preferred to other measures of central tendency when your data set is skewed. The median is a better measure of the typical amount of time Americans spend on email weekly.

How about the confidence interval of email usage? Let's take a look.

```{r email_boot_ci}

# seed set above

# bootstrap for MEAN email minutes
boot_email <- gss %>%
  # Specify the variable of interest
  specify(response = email) %>%
  
  # Generate a bunch of bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>%
  
  # Find the median of each sample
  calculate(stat = "mean")

percentile_ci <- boot_email %>%
  get_ci(level = 0.95, type = "percentile")

# print confidence interval
percentile_ci %>% 
  mutate(lower_emailhr = lower_ci %/% 60, 
         lower_emailmin = round(lower_ci %% 60),
         upper_emailhr = upper_ci %/% 60, 
         upper_emailmin = round(upper_ci %% 60),
         `lower bound of average email usage` = paste(lower_emailhr, "hr, ", lower_emailmin, "min"),
         `upper bound of average email usage` = paste(upper_emailhr, "hr, ", upper_emailmin, "min")) %>% 
  select(`lower bound of average email usage`, `upper bound of average email usage`) %>% # only show relevant columns
  kbl(caption = "Confidence interval for weekly email usage") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

# visualise confidence interval
ggplot(boot_email, aes(x = stat)) +
  geom_histogram(bins = 20) +
  labs(title= "Bootstrap distribution of means") +
  geom_vline(xintercept = percentile_ci$lower_ci, colour = 'orange', linetype = 2, size = 1.2) +
  geom_vline(xintercept = percentile_ci$upper_ci, colour = 'orange', linetype = 2, size = 1.2) +
  theme_bw()

```

We can find the confidence interval of weekly time spend on email under 95% is [6 hr 26 min, 7 hr 30 min]. Americans spend around 1 hour per day on email!
However, as we have seen in the distribution plot, this is probably only due to some people spending the whole day emailing, while others only check their mails once in a while. 

We would expect a 99% confidence interval to be wider than the interval calculated because to be more confident that the true population value falls within the interval we will need to allow more potential values within the interval.


# Trump's Approval Margins

As we saw in class, fivethirtyeight.com has detailed data on [all polls that track the president's approval ](https://projects.fivethirtyeight.com/trump-approval-ratings). Let's see what we can learn from the data.

To do this, we first have to load the data:

```{r, cache=TRUE}

# Import approval polls data
approval_polllist <- read_csv(here::here('project_data', 'approval_polllist.csv'))

approval_polllist <- approval_polllist %>% 
  # correctly interpret dates and timestamps (datetimes) with the help of lubridate
  mutate(modeldate = mdy(modeldate),
         startdate = mdy(startdate),
         enddate = mdy(enddate),
         createddate = mdy(createddate),
         timestamp = parse_date_time(timestamp, orders="hms dmy")
         )

glimpse(approval_polllist)
```

## Create a plot

As a next step, we calculate the average net approval rate (approve - disapprove) for each week since he got into office.  

Then, we can plot this net approval alongside 95% confidence intervals.

```{r trump_margins_recreation, fig.height=6.5, fig.width=15}
# watch out for the chunk options!! (fig.height and fig.width)

# wrangle the data and calculate the CIs
approval_polllist_aggr <- approval_polllist %>% 
  # calculate the approval rate and get year and week
  mutate(approval_rate = approve - disapprove,
         year = factor(year(enddate)),
         week = isoweek(enddate)) %>% 
  # group by year and week and calculate the CI values
  group_by(year, week) %>% 
  summarise(average_approval_rate = mean(approval_rate), # calculate average approval rate per week
            SD = sd(approval_rate), # calculate SD
            count = n(), # calculate count
            t_critical = qt(0.975, count - 1), # get t-critical value with (count-1) degrees of freedom
            SE =  SD/sqrt(count), # calculate standard error
            margin_of_error = t_critical * SE, # calculate margin of error
            lower_ci = average_approval_rate - margin_of_error, # calculate lower bound of CI
            upper_ci = average_approval_rate + margin_of_error) # calculate upper bound of CI

# reproduce the plot
ggplot(approval_polllist_aggr, aes(x = week, color = year, fill = year)) +
  geom_line(aes(y = average_approval_rate)) + # add line for the average approval
  geom_point(aes(y = average_approval_rate)) + # add points for the average approval
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci),alpha=0.1) + # add the CI ribbon
  geom_hline(aes(yintercept = 0), color = "orange") + # add the horizontal line
  facet_wrap(~year) + # faceting by year
  theme_bw() + # nice theme
  labs(y = "Average Net Approval (%)",
       x = "Week of the year",
       title = "Estimating Net Approval (approve-disapprove) for Donald Trump",
       subtitle = "Weekly average of all polls") +
  scale_x_continuous(breaks = c(0, 13, 26, 39, 52)) + # specific breaks
  scale_y_continuous(breaks = c(-20, -17.5, -15, -12.5, -10, -7.5, -5, -2.5, 0, 2.5, 5, 7.5)) + # specific breaks
  theme(legend.position = "none", # delete legend
        axis.title = element_text(size = 15.5), # change size of axis titles,
        axis.text = element_text(size = 13), # changes size of axis labels,
        strip.text = element_text(size = 14), # change size of facet titles
        plot.subtitle = element_text(size = 15), # change size of plot subtitle
        plot.title = element_text(size = 19)) # change size of plot title

```

Original Plot:

```{r trump_margins, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "trump_approval_margin.png"), error = FALSE)
```

## Compare Confidence Intervals

Let's have a deeper look and compare the confidence intervals for `week 15` (6-12 April 2020) and `week 34` (17-23 August 2020):

```{r}

approval_polllist_aggr %>% 
  filter(week %in% c(15, 34), year == 2020) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

```

We can clearly see, that the confidence interval for week 34 is much wider, i.e. the margin of error is much bigger.
This is due to the following reasons:

1. We have less observations (count is lower)
2. The standard deviation for week 34 is higher, meaning that different polls don't really agree on a approval rate
3. These two factors together lead to a higher standard error and a higher t_critical, and thus to a higher margin of error


# Gapminder revisited

Recall the `gapminder` data frame from the gapminder package. That data frame contains just six columns from the larger [data in Gapminder World](https://www.gapminder.org/data/). In this part, we will join a few dataframes with more data than the 'gapminder' package. They are 

- HIV prevalence (adults_with_hiv_percent_age_15_49.csv): The estimated number of people living with HIV per 100 population of age group 15-49.
- Life expectancy at birth (life_expectancy_years.csv)
- The World Bank's data of GDP per capita in constant 2010 US$, Female fertility _(The number of babies per woman)_, Primary school enrollment _(% of children attending primary school)_, Mortality rate _(for under 5, per 1000 live births)_.

Firstly, we download the raw data.

```{r data_download, get_data, cache=TRUE}

# load gapminder HIV data
hiv <- read_csv(here::here("project_data","adults_with_hiv_percent_age_15_49.csv"))
life_expectancy <- read_csv(here::here("project_data","life_expectancy_years.csv"))

# get World bank data using wbstats
indicators <- c("SP.DYN.TFRT.IN","SE.PRM.NENR", "SH.DYN.MORT", "NY.GDP.PCAP.KD")

library(wbstats)

worldbank_data <- wb_data(country="countries_only", #countries only- no aggregates like Latin America, Europe, etc.
                          indicator = indicators, 
                          start_date = 1960, 
                          end_date = 2016) %>% 
                  rename(fertility = SP.DYN.TFRT.IN,
                         GDP_cap = NY.GDP.PCAP.KD,
                         prim_enrol = SE.PRM.NENR,
                         mortality = SH.DYN.MORT) # rename the col to make it more readable

# get a dataframe of information regarding countries, indicators, sources, regions, indicator topics, lending types, income levels,  from the World Bank API 
countries <-  wbstats::wb_cachelist$countries

```

Then, let's join the 3 dataframes (life_expectancy, worldbank_data, and HIV) into one. 

Since each dataset varies in the countries and years they covered, the method used here is full join in order NOT to delete any information. However if we keep all data, there might be NAs, but we can later on still filter out NAs, but ONLY FOR THE COLUMNS WE ANALYSE!

In addition, for further grouping analysis purpose, we also match additional information to each country using a left join.
In this case, it is a left join, because we do only need additional information for the countries we already have in our dataset.

```{r tidy_n_join}
library(countrycode)
library(gapminder)

# tidy data

life_expectancy_long <- life_expectancy %>%
  # bring into long format
  pivot_longer(cols = '1800':'2100', names_to = "date", values_to = "life_exp") %>%
  mutate(date = as.double(date),
         iso3c = countrycode(country, # get iso3c code
                             origin = 'country.name',
                             destination = 'iso3c'))

hiv_long <- hiv %>%
  # bring into long format
  pivot_longer(cols = '1979':'2011', names_to = "date", values_to = "hiv_prev") %>%
  mutate(date = as.double(date),
         iso3c = countrycode(country, # get iso3c code
                             origin = 'country.name', 
                             destination = 'iso3c'))

#join the 3 dataframes (life_expectancy, worldbank_data, and HIV) into one
joined_df <- full_join(subset(life_expectancy_long, select = -c(country)),
                       full_join(subset(hiv_long, select = -c(country)),
                                 subset(worldbank_data, select = -c(iso2c),
                                 by = c("iso3c","date"))),
                       by = c("iso3c","date"))

# add additional info via left join
joined_df_region <- joined_df %>% 
  left_join(subset(countries, select = -c(country)), by = "iso3c")
  

```

Now we have all the data in hand, including HIV prevalence, Life expectancy, GDP per capita, Female fertility, Primary school enrollment, and Mortality rate. Let's check they relationships one by one.

## HIV prevalence and life expectancy, by region

Our first question would be - What is the relationship between HIV prevalence and life expectancy?

```{r hiv_life_exp}

joined_df %>%
  drop_na(c(hiv_prev,life_exp)) %>%
  ggplot(aes(x = hiv_prev/100, y = life_exp)) + # hiv_prevalence is "The estimated number of people living with HIV per 100 population of age group 15-49."
  geom_point(alpha = 0.2) +
  geom_smooth()+ # added smoothed line
  scale_x_continuous(labels = scales::percent) + # scale with percentages
  theme_bw() + 
  labs(title = "Lower life expectancy with higher HIV prevalence",
       subtitle = "Life expectancy in different countries and years by HIV prevalence",
       x = "HIV prevalence in age group 15-49",
       y = "life expectancy",
       caption = "Source: World Bank")+
  NULL

joined_df_region %>%
  drop_na(c(hiv_prev,life_exp)) %>%
  filter(region %in% c("Latin America & Caribbean", "Sub-Saharan Africa")) %>%
  ggplot(aes(x = hiv_prev/100, y = life_exp)) + # hiv_prevalence is "The estimated number of people living with HIV per 100 population of age group 15-49."
  geom_point(alpha = 0.2) +
  geom_smooth()+ # add smoothed line
  scale_x_continuous(labels = scales::percent) + # scale with percentages
  theme_bw() + 
  labs(title = "Lower life expectancy with higher HIV prevalence",
       subtitle = "Life expectancy in different countries and years by HIV prevalence",
       x = "HIV prevalence in age group 15-49",
       y = "life expectancy",
       caption = "Source: World Bank")+
  facet_wrap(~region, scales = "free")+
  NULL
  
```

From a general view, we know in the range between 0%-7% of HIV prevalence, the life expectancy decrease pretty quickly as HIV prevalence raises. When HIV prevalence exceed 7%, the life expectancy doesn't significantly continue to decrease, because of few observations and the fact that ~50 is already the worst life expectancy for HIV.

Zooming in to see what's going on for specific regions, the results are similar.

## Fertility rate and GDP per capita, by region

We're also curious about the the relationship between fertility rate and GDP per capita. Let's take a look.

```{r fertility_GDP_cap}

joined_df %>%
  drop_na(c(fertility, GDP_cap)) %>%
  ggplot(aes(x = fertility, y = GDP_cap)) +
  geom_point(alpha = 0.2) +
  geom_smooth()+ # add smoothed line
  scale_y_log10() + # logarithmic scale
  theme_bw() + 
  labs(title = "Lower GDP per capita with higher fertility",
       subtitle = "GDP per capita in different countries and years by fertility",
       x = "fertility (no. of babies per woman)",
       y = "GDP per capita (log scale)",
       caption = "Source: World Bank")+
  NULL

joined_df_region %>%
  drop_na(c(fertility, GDP_cap)) %>%
  ggplot(aes(x = fertility, y = GDP_cap)) + 
  geom_point(alpha = 0.2) +
  geom_smooth() + # add smoothed line
  scale_y_log10() + # logarithmic scale
  theme_bw() + 
  labs(title = "Lower GDP per capita with higher fertility",
       subtitle = "GDP per capita in different countries and years by fertility",
       x = "fertility (no. of babies per woman)",
       y = "GDP per capita (log scale)",
       caption = "Source: World Bank")+
  facet_wrap(~region, scales = "free")+ # faceting by region
  NULL

```

Not surprisingly, the lower GDP per capita, the higher fertility. There are complicated economics and sociology reasons behind it, such as hoping for demographic dividend to drive the GDP, and stronger social welfare system decreasing the need of children support when retired.

For regions having more 3rd world countries, the trend are more rapid, corresponding to their economics growth.

## Missing HIV data by region

When analyzing the HIV prevalence, we found a lot missing values. Which regions have the most observations with missing HIV data? 

```{r missing_hiv_Data}

hiv_long_region <- left_join(hiv_long, 
                             countries,
                             by = "iso3c")

hiv_long_region %>%
  group_by(region) %>%
  summarise(num_na = sum(is.na(hiv_prev))) %>%
  ggplot(aes(x = num_na, y = reorder(region,num_na))) +
  geom_col() +
  theme_bw() + 
  labs(y = "", 
       x = "Count of observations with missing HIV data",
       title = "Observations with missing HIV data by region")

```

## Change of mortality rate for under 5, by region

How has mortality rate for under 5 changed by region? We plot the overall picture and the top/bottom five countries in each region.

```{r mortality_rate_change}

joined_df_region %>%
  drop_na(mortality) %>%
  ggplot(aes(x = date, y = mortality)) +
  theme_bw() +
  geom_line(aes(group = iso3c, color = iso3c)) + # add lines
  theme(legend.position = "none") +
  labs(title = "Mortality for under 5 dropped in all regions!",
     subtitle = "Development of mortality (under 5) in different regions over the past years",
     x = "",
     y = "mortality for under 5, per 1000 live births") +
  facet_wrap(~region) + # faceting by region
  NULL

# calculate improvement in mortality per country
mortality_ranking <- joined_df_region %>%
  drop_na(mortality) %>%
  arrange(iso3c, date) %>%
  group_by(region, country) %>%
  summarise(early_m = first(mortality), # get the earliest recorded value in mortality
            latest_m = last(mortality), # get the latest recorded value in mortality
            improvement_m =  - (latest_m - early_m)/early_m) %>% # calculate difference / improvement
  arrange(region, desc(improvement_m))

# Top 5

mortality_ranking %>% 
  top_n(5, improvement_m) %>%
  select('region','country','improvement_m') %>%
  rename('top 5 countries' = country,
         'mortality improvement' = improvement_m) %>% # have readable names
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

# Bottom 5

mortality_ranking %>% 
  top_n(-5, improvement_m) %>%
  select('region','country','improvement_m') %>%
  arrange(region, improvement_m) %>%
  rename('bottom 5 countries' = country,
         'mortality improvement' = improvement_m) %>% # have readable names
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) # have a nice HTML table

```

## Primary school enrollment and fertility rate

Is there a relationship between primary school enrollment and fertility rate?

```{r primary_enrollment_fertility}

joined_df_region %>%
  drop_na(c(prim_enrol,fertility)) %>%
  ggplot(aes(x = prim_enrol/100, y = fertility))+
  geom_point(alpha = 0.2)+
  geom_smooth()+ # add smoothed line
  theme_bw()+
  labs(title = "The more people are engaged in primary education, \nthe less is the fertility rate",
       subtitle = "Fertility under different primary school enrolment rate by countries and years",
       x = "primary school enrolment rate",
       y = "fertility (no. of babies per woman)",
       caption = "Source: World Bank") +
  scale_x_continuous(label = scales::percent) + # have percentages
  NULL

```

High levels of primary school enrollment are associated with fertility rate declines. This indicates a decline in patriarchy, change in women's familial roles and a shift to a more egalitarian family structure. It would be better for us to gain women school enrollment, because in general increases in female education mean higher status and more power for women. 


# Challenge 1: CDC COVID-19 Public Use Data

Let us revisit the [CDC Covid-19 Case Surveillance Data](https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf). There are well over 3 million entries of individual, de-identified patient data. Sounds great? Of course! Let's load that data using `vroom`:


```{r, cache=TRUE}
# URL link to CDC to download data
url <- "https://data.cdc.gov/api/views/vbim-akqf/rows.csv?accessType=DOWNLOAD"

covid_data <- vroom(url)%>%
  clean_names()


```

Replicated plots:

```{r covid_challenge_reproduction, fig.height=8, fig.width=15}
# watch out for the chunk options!! (fig.height and fig.width)

covid_data <- covid_data %>% 
  # clean death rate
  mutate(death_yn_clean = case_when(death_yn == "Yes" ~ "Yes",
                                    death_yn == "No" ~ "No",
                                    TRUE ~ NA_character_)
  ) %>% 
  # clean age group
  mutate(age_group_clean = ifelse(age_group == "Unknown", NA_character_, age_group)) %>% 
  # clean sex
  mutate(sex_clean = case_when(sex == "Male" ~ "Male",
                               sex == "Female" ~ "Female",
                               TRUE ~ NA_character_)
  ) %>% 
  # clean co-morbidities
  mutate(medcond_yn_clean = factor(case_when(medcond_yn == "Yes" ~ "Yes",
                                             medcond_yn == "No" ~ "No",
                                             TRUE ~ NA_character_), 
                                   levels = c("Yes", "No"),
                                   labels = c("With comorbidities", "Without comorbidities"))
  ) %>% 
  # clean ICU status
  mutate(icu_yn_clean = factor(case_when(icu_yn == "Yes" ~ "Yes",
                                         icu_yn == "No" ~ "No",
                                         TRUE ~ NA_character_),
                               levels = c("Yes", "No"),
                               labels = c("Admitted to ICU", "No ICU"))
  )
  

# first graph
covid_data %>% 
  # only work with rows we have data for
  drop_na(c(age_group_clean, sex_clean, medcond_yn_clean, death_yn_clean)) %>% 
  # group by and calculate the death rate per subgroups
  group_by(age_group_clean, sex_clean, medcond_yn_clean) %>% 
  summarise(n_yes = count(death_yn_clean == "Yes"),
            n_no = count(death_yn_clean == "No"),
            death_rate = n_yes / (n_yes + n_no)) %>% 
  # plot the calculated death rates per subgroup
  ggplot(aes(y = age_group_clean, x = death_rate)) +
  geom_col(fill = "#6B7CA4") + # set bar color
  geom_text(aes(label = round(100*death_rate, 1)), # add the labels next to the bars
            position = position_dodge(width = .9),    # move to center of bars
            hjust = -0.1, # nudge position right to bars
            size = 3) + 
  facet_grid(rows = vars(medcond_yn_clean),
             cols = vars(sex_clean)) + # faceting by sex and medcond
  scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.6), labels = scales::percent) + # show as percentages and have individual breaks
  labs(title = "Covid death % by age group, sex and presence of co-morbidities",
       caption = "Source: CDC",
       x = NULL,
       y = NULL) +
  theme_bw() + # nice theme
  theme(axis.text = element_text(size = 6), # changes size of axis labels,
        strip.text = element_text(size = 6), # change size of facet titles
        plot.caption = element_text(size = 6), # change size of plot subtitle
        plot.title = element_text(size = 8)) # change size of plot title
  

  
# second graph
covid_data %>% 
  # only work with rows we have data for
  drop_na(c(age_group_clean, sex_clean, icu_yn_clean, death_yn_clean)) %>% 
  # group by and calculate the death rate per subgroups
  group_by(age_group_clean, sex_clean, icu_yn_clean) %>% 
  summarise(n_yes = count(death_yn_clean == "Yes"),
            n_no = count(death_yn_clean == "No"),
            death_rate = n_yes / (n_yes + n_no)) %>% 
  # plot the calculated death rates per subgroup
  ggplot(aes(y = age_group_clean, x = death_rate)) +
  geom_col(fill = "#FF9582") + # set bar color
  geom_text(aes(label = round(100*death_rate, 1)),# add the labels next to the bars
            position = position_dodge(width = .9), # move to center of bars
            hjust = -0.1,    # nudge position right to bars
            size = 3) + 
  facet_grid(rows = vars(icu_yn_clean), 
             cols = vars(sex_clean)) + # faceting by icu and sex
  scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.6, 0.8), labels = scales::percent) + # show as percentages and have individual breaks
  labs(title = "Covid death % by age group, sex and whether patient was admitted to ICU",
       caption = "Source: CDC",
       x = NULL,
       y = NULL) +
  theme_bw() + # nice theme
  theme(axis.text = element_text(size = 6), # changes size of axis labels,
        strip.text = element_text(size = 6), # change size of facet titles
        plot.caption = element_text(size = 6), # change size of plot subtitle
        plot.title = element_text(size = 8)) # change size of plot title
  

```

Original plots:

```{r covid_challenge, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "covid_death_rate_comorbidities.png"), error = FALSE)
knitr::include_graphics(here::here("images", "covid_death_rate_icu.png"), error = FALSE)
```


# Challenge 2: Excess rentals in TfL bike sharing

Recall the TfL data on how many bikes were hired every single day. We can get the latest data by running the following

```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```

We can easily create a facet grid that plots bikes hired by month and year:

```{r tfl_month_year_grid, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

Looking at May and Jun and comparing 2020 with the previous years, we can directly see the impact of the COVID pandemic!

Original Plot:

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```

Plot Reproduction:

```{r monthly_changes, fig.height=5.5, fig.width=15}

# data preparation

expected_rentals_df <- bike %>% 
  # use years 2015-2019 as reference
  filter(year %in% 2015:2019) %>% 
  # calculate expected rentals (per day) per month over all years
  group_by(month) %>% 
  summarise(expected_rentals = mean(bikes_hired))

excess_by_month <- bike %>% 
  # use years 2015-2019 as reference
  filter(year %in% 2015:2020) %>% 
  # calculate actual rentals for each month for every year
  group_by(year, month) %>% 
  summarise(actual_rentals = mean(bikes_hired)) %>% 
  # join the expected rentals per month (over all years)
  left_join(expected_rentals_df, on = "month") %>% 
  # calculate excess_rentals and helper variables for the ribbon
  mutate(excess_rentals = actual_rentals - expected_rentals,
         excess_rentals_pos = ifelse(excess_rentals>0, excess_rentals+expected_rentals, expected_rentals),
         excess_rentals_neg = ifelse(excess_rentals<0, excess_rentals+expected_rentals, expected_rentals))


# plot graph 1
ggplot(excess_by_month, aes(x = month)) +
  # add line for expected rentals (same for all years)
  geom_line(aes(y = expected_rentals), group = 1, size = 1, color = "blue") +
  # add black line for actual rentals 
  geom_line(aes(y = actual_rentals), group = 1, color = "black") +
  # add red and green ribbons
  geom_ribbon(aes(ymin = expected_rentals, ymax = excess_rentals_pos, group = 1), fill = "green4", alpha = 0.2) +
  geom_ribbon(aes(ymin = expected_rentals, ymax = excess_rentals_neg, group = 1), fill = "red3", alpha = 0.2) +
  # faceting by year
  facet_wrap(~year) +
  theme_minimal() +
  # use specific breaks
  scale_y_continuous(breaks = c(20000, 25000, 30000, 35000, 40000)) +
  labs(y = "Bike rentals",
       x = "",
       title = "Monthly changes in TfL bike rentals",
       subtitle = "Change from monthly average shown in blue \nand calculated between 2015-2019",
       caption = "Source: TfL, London Data Store") +
  theme(axis.text = element_text(size = 12), # changes size of axis labels,
        axis.title = element_text(size = 15), # changes size of axis titles,
        strip.text = element_text(size = 13), # change size of facet titles
        plot.subtitle = element_text(size = 15), # change size of plot subtitle
        plot.caption = element_text(size = 12), # change size of plot caption
        plot.title = element_text(size = 18)) # change size of plot title

```

Original Plot:

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```

Plot Reproduction:

```{r weekly_changes, fig.height=6.5, fig.width=15}

# data preparation

expected_rentals_df_2 <- bike %>% 
  # use years 2015-2019 as reference
  filter(year %in% 2015:2019) %>% 
  # calculate expected rentals (per day) per week over all years
  group_by(week) %>% 
  summarise(expected_rentals = mean(bikes_hired))

excess_by_week <- bike %>% 
  # use years 2015-2019 as reference
  filter(year %in% 2015:2020) %>% 
  # calculate actual rentals for each week for every year
  group_by(year, week) %>% 
  summarise(actual_rentals = mean(bikes_hired)) %>% 
  # join the expected rentals per week (over all years)
  left_join(expected_rentals_df_2, on = "week") %>% 
  # calculate excess_rentals, % change from weekly averages and helper variables for the ribbon and the rugs
  mutate(excess_rentals = actual_rentals - expected_rentals,
         perc_change = excess_rentals / expected_rentals,
         perc_chane_pos = ifelse(perc_change>0, perc_change, 0),
         perc_chane_neg = ifelse(perc_change<0, perc_change, 0),
         pos_neg = factor(ifelse(perc_change > 0, 1, 0)))


# plot graph 2
ggplot(excess_by_week, aes(x = week)) +
  # add grey shaded rectangles
  geom_tile(aes(x = 19.5, width = 13, y = 0, height = Inf), alpha = 0.01, fill = "gray78") +
  # add grey shaded rectangles
  geom_tile(aes(x = 46, width = 14, y = 0, height = Inf), alpha = 0.01, fill = "gray78") +
  # add black line for percentage change
  geom_line(aes(y = perc_change), group = 1, color = "black") +
  # add rugs at the bottom
  geom_rug(aes(color = pos_neg)) +
  # add red and green ribbons
  geom_ribbon(aes(ymin = 0, ymax = perc_chane_pos, group = 1), fill = "green4", alpha = 0.2) +
  geom_ribbon(aes(ymin = perc_chane_neg, ymax = 0, group = 1), fill = "red3", alpha = 0.2) +
  # faceting by year
  facet_wrap(~year) +
  theme_minimal() +
  # use specific breaks
  scale_y_continuous(labels = scales::percent, breaks = c(-0.6, -0.3, 0, 0.3, 0.6), minor_breaks = c(-0.45, -0.15, 0.15, 0.45)) +
  scale_x_continuous(breaks = c(13, 26, 39, 53), minor_breaks = c(0, 6.5, 19.5, 32.5, 45.5)) +
  scale_color_manual(values=c("red3", "green4")) + # change colors of rugs
  theme(legend.position = "none") + # without legend
  labs(y = "",
       x = "week",
       title = "Weekly changes in TfL bike rentals",
       subtitle = "% change from weekly averages \ncalculated between 2015-2019",
       caption = "Source: TfL, London Data Store") +
  theme(axis.text = element_text(size = 12), # changes size of axis labels,
        axis.title = element_text(size = 15), # changes size of axis titles,
        strip.text = element_text(size = 13), # change size of facet titles
        panel.grid.minor = element_line(colour = "gray90"), # change color of minor grid
        panel.grid.major = element_line(colour = "gray90"), # change color of major grid
        plot.subtitle = element_text(size = 15), # change size of plot subtitle
        plot.caption = element_text(size = 12), # change size of plot caption
        plot.title = element_text(size = 18)) # change size of plot title

```



# Details

- Who did you collaborate with: Noor Alameri, Brigita Angkasa, Lujia Huang, Martino Armanini, Marco Laube, Deniz Oezdemir
- Approximately how much time did you spend on this problem set: 12h
- What, if anything, gave you the most trouble: The formatting in the two Challenges


# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.