---
challenge: Study Group 10
client: Jane Doe
date: "2020-09-06"
service: Data Analytics
shortDescription: xx
solution: xx
thumbnail: images/portfolio/0.png
title: homework 1
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(kableExtra)

options(scipen=200)
```



# Where Do People Drink The Most Beer, Wine And Spirits?

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alcohol consumption in different countries. The data `drinks` is available as part of the `fivethirtyeight` package. Let's load the data and have a look!


```{r load_alcohol_data}
library(fivethirtyeight)
data(drinks)
```


To get an idea of what the data is like, we use `glimpse` and `skim`. Furthermore, we can use `?drinks` to get some more meta information.

```{r glimpse_skim_data}
glimpse(drinks)
skim(drinks)
# ?drinks
```

We find out that we deal with a table with 193 rows, where each row represents one specific `country`. The country names are saved by their English names, which is why the variable `country` is of type `character`.
Additionally, we have three columns `beer_servings`, `spirit_servings` and `wine_servings` of type `integer` (i.e. numeric values), that
save the servings of beer, spirits and wine in average serving sizes per person.
Finally, we have a `double` variable `total_litres_of_pure_alcohol`, which measures the total litres of pure alcohol consumed per person.  

With the help of the `skim` output we can see that there are no missing values in any column (look for `n_missing`).  

Having understood the structure of our data, we can proceed to actually answer our question of where people do drink the most wine, beer and spirits. Please have a look at the following graphs.

```{r beer_plot}

top_25_beer <- drinks %>% 
  top_n(25, beer_servings) # filter for the top 25 beer consuming countries

ggplot(top_25_beer, aes(x = beer_servings, 
                        y = reorder(country, beer_servings))) + # reorder bars by beer servings
  geom_col(fill = "goldenrod2") + # color of bars represents the color of a nice beer
  labs(title = "Namibian people drink more beer than anybody else!",
       subtitle = "Beer consumption per country.",
       caption = "Source: Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?",
       x = "Servings of beer in average serving sizes per person",
       y = "Country") +
  NULL

```


```{r wine_plot}

top_25_wine <- drinks %>% 
  top_n(25, wine_servings) # filter for the top 25 wine consuming countries

ggplot(top_25_wine, aes(x = wine_servings, 
                        y = reorder(country, wine_servings))) + # reorder bars by wine servings
  geom_col(fill = "red4") + # color of bars represents the color of a nice red wine
  labs(title = "French people are the greatest wine enthusiasts!",
       subtitle = "Wine consumption per country.",
       caption = "Source: Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?",
       x = "Servings of wine in average serving sizes per person",
       y = "Country") +
  NULL

```

```{r spirits_plot}

top_25_spirits <- drinks %>% 
  top_n(25, spirit_servings) # filter for the top 25 spirits consuming countries

ggplot(top_25_spirits, aes(x = spirit_servings, 
                        y = reorder(country, spirit_servings))) + # reorder bars by spirits servings
  geom_col(fill = "white", color = "black") + # color of bars represents the color of a nice vodka
  labs(title = "The Caribbean and the Eastern Bloc top the spirits list!",
       subtitle = "Spirits consumption per country.",
       caption = "Source: Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?",
       x = "Servings of spirits in average serving sizes per person",
       y = "Country") +
  NULL

```

Regarding the consumption of beer, we expected to see Germany, Poland and the Czech Republic at the top of the list, as they
are well known for their beer culture. However, at first glance it is quite surprising that Namibia and Gabon are on the winners' podium.
For Namibia, one explanation could be that it was a German colony for a long time. But an even more persuasive argument, that also applies to
Gabon, is that both countries mainly consume alcohol in the form of beer:

```{r african_beer_inspection}

drinks %>% 
  filter(country == "Gabon" | country == "Namibia") %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

So as supplies in spirits and wine are (probably) low, they fall back on beer.  

Finally, Namibia has numerous beer-filled events throughout the year and has a fast growing emerging crafting beer market, which support the fact that it is the country with the highest beer servings.

Regarding the consumption of wine, there are no real surprisings. Drinking wine is in the heart of the local culture in France. On top of that, wine is also associated with tradition and sophistication of French culture.  

Finally, the Russian Federation and Eastern Europe are well known
for their huge consumption of Vodka. But we also see that countries in the Caribbean drink quite a lot of spirits! This is probably because
of the liquid gold of the Caribbean: Rum.


# Analysis of movies - IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset).
Let's load the data and have a look at it!

  
```{r load_inspect_movies, warning=FALSE, message=FALSE}

movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)
skim(movies)

```

Besides the obvious variables of `title`, `genre`, `director`, `year`, and `duration`, the rest of the variables are as follows:

- `gross` : The gross earnings in the US box office, not adjusted for inflation
- `budget`: The movie's budget 
- `cast_facebook_likes`: the number of facebook likes cast members received
- `votes`: the number of people who voted for (or rated) the movie in IMDB 
- `reviews`: the number of reviews for that movie
- `rating`: IMDB average rating 

Thanks to the `skim` output, we can see that we are dealing with a clean dataset, where no values are missing (watch out for `n_missing`).

However, we see that some `title`s, `genre`s and `director`s occur more than once (watch out for `n_unique`). This is quite
understandable for `genre` and `director`, but not so much for `title`s.
Let's investigate this issue:

```{r get_duplicated_movies}

movies %>% 
  get_dupes(title) %>% 
  head(6) %>% 
  rename(`cast facebook likes` = cast_facebook_likes) %>% 
  select(-dupe_count) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

We really have some movies that appear twice in the list! Note that there are always slight differences in the `votes` or `cast_facebook_likes`
variables for the entries of the same movie. As we are only interested in analysing unique movies, we drop duplicated movies.
Here, we determine a unique movie by the combination of its title, genre, director, and year.

```{r movie_deduplication}

unique_movies <- movies %>% 
  distinct(title, genre, director, year, .keep_all = TRUE) # only keep unique movies
  
unique_movies %>% 
  count() %>% 
  rename(`number of unique movies:` = n) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

We see that we end up with 2907 unique movies, which is exactly the number of unique movie titles we identified using `skim`.

## Analysis

With this cleaned dataset, we can now start our analysis!
Let's first explore, how many movies we deal with by genre:

```{r movies_by_genre}

unique_movies %>% 
  group_by(genre) %>% 
  count() %>% # count number of movies by genre
  rename(`number of movies` = n) %>% 
  arrange(desc(`number of movies`)) %>%  # descending order
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

Moreover, we want to analyse the average gross earning and budget by genre:

```{r avg_by_genre}

unique_movies %>% 
  group_by(genre) %>% 
  summarise(avg_gross_earning = mean(gross), 
            avg_budget = mean(budget)) %>% # compute summary statistics by genre
  rename(`average gross earnings` = avg_gross_earning, `average budget` = avg_budget) %>% 
  arrange(desc(`average gross earnings`)) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

We can see that for example Musicals have one of the highest average gross earnings, whilst having one of the lowest average budgets.
To dig deeper, we want to create a variable `return_on_budget`, which shows how many $ a movie made at the box office for each $ of its budget.
Calculating the average return on budget, we obtain a new ranking:

```{r return_on_budget}

unique_movies %>% 
  mutate(retun_on_budget = gross/budget) %>% # compute new variable return_on_budget
  group_by(genre) %>% 
  summarise(`average gross earnings` = mean(gross), 
            `average budget` = mean(budget), 
            `average return on budget` = mean(retun_on_budget)) %>% # compute summary statistics by genre
  mutate(rank = rank(desc(`average return on budget`))) %>% # compute rank of genre based on average return on budget (higher is better)
  arrange(rank) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

If you are interested in making some money with movie making, you should definitely shoot a horror movie!
But with which director should you work? Let's find out, which 15 directors have created the highest gross revenue in the box office:

```{r best_directors}

unique_movies %>% 
  group_by(director) %>% 
  summarise(`total gross` = sum(gross), 
            `mean gross`= mean(gross), 
            `median gross` = median(gross), 
            `sd of gross` = sd(gross))%>% # compute summary statistics by director
  top_n(15, `total gross`) %>% # filter for the 15 directors with the highest total gross revenue in the box office 
  arrange(desc(`total gross`)) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

Now that we know about the financial side and which directors have created the highest gross revenue in total, we are also interested in how
movies are rated across genre. Is there a genre, that is everybody's darling?

```{r rating_by_genre}

unique_movies %>% 
  group_by(genre) %>% 
  summarise(`min rating` = min(rating), 
            `median rating` = median(rating), 
            `average rating` = mean(rating), 
            `max rating` = max(rating),
            `sd of rating` = sd(rating)) %>% # compute summary statistics by genre
  arrange(desc(`average rating`)) %>% # order by descending average rating
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

Finally, we can visually show the distribution of the ratings with different plots:

```{r rating_plots}

ggplot(unique_movies, aes(x = rating, y = genre)) + 
  geom_boxplot() +
  labs(title = "Boxplot of ratings by genre")

ggplot(unique_movies, aes(x = rating)) + 
  geom_density() +
  labs(title = "Distribution of ratings over all genres")

ggplot(movies, aes(x = rating)) +
  geom_density() +
  facet_wrap(~genre) + # show density separately for each genre
  labs(title = "Distribution of ratings by genre")

```

Watch out that for the genres of musicals, romances, westerns, thrillers and family movies we have too less observations for the density plots to be reliable!

We now have an idea of the financial side and the ratings by genre. Furthermore, we know the top directors.
To dig even deeper, we are going to analyse the relationship between different variables.

We might have the assumption that a famous cast brings in more money. Let's see if our gut feeling is right!

```{r, gross_on_fblikes_1}

ggplot(unique_movies, aes(x = cast_facebook_likes, y = gross)) + 
  geom_point(alpha = 0.2) +
  labs(x = "number of facebook likes cast members received",
       y = "earnings in the US box office")

```

Well, this graph is not really helpful. Let's see if we can improve it with logarithmic scales:


```{r, gross_on_fblikes_2}

ggplot(unique_movies, aes(x = cast_facebook_likes, y = gross)) + 
  geom_point(alpha = 0.3) + # set opacity of the points
  geom_smooth() + # add smoothed conditional means
  scale_y_log10() + # get a logarithmic scale for the y axis
  scale_x_log10() + # get a logarithmic scale for the x axis
  labs(x = "number of facebook likes cast members received",
       y = "earnings in the US box office")

```

Thanks to the logarithmic scale, we can see that indeed there is a slight tendency that a cast with more facebook likes leads to higher 
earnings in the box office. However, this pattern is not really clear in the scatterplot, which is why `cast_facebook_likes` is rather bad in
predicting `gross`. This is reinforced by a low correlation of only 0.213:

```{r facebook_cor}

cor(movies$gross, movies$cast_facebook_likes)

```

Probably, it is far more important how big the budget of a movies is. Special effects and a beautiful scenery are probably crucial
for a movie to produce high earnings.

```{r, gross_on_budget}

ggplot(unique_movies, aes(x = budget, y = gross)) + 
  geom_point(alpha = 0.2) + # set opacity of the points
  geom_smooth() + # add smoothed conditional means
  labs(x = "budget",
       y = "earnings in the US box office")

cor(movies$gross, movies$budget) # calculate correlation of gross and budget


```

In this case, the relationship is far more apparent. Even though there are still other factors affecting earnings, the budget seems like
a proper predictor of how much money a movie will make at the box office.

Finally, we want to examine the relationship between `gross` and `rating`.

```{r, gross_on_rating}

ggplot(unique_movies, aes(x = rating, y = gross)) + 
  geom_point(alpha = 0.2) + # set opacity of points
  labs(x = "rating",
       y = "earnings in the US box office") + 
  geom_smooth() +
  facet_wrap(~genre) # show scatterplot for each genre

```

For the genres of action, adventure, biography, comedy, crime, drama and horror, we can see that high earnings only come with higher ratings.
However, there are also a lot of well rated movies, that did not have huge earnings in the US box office. Hence, it is not optimal to take
`rating` as a good predictor for `gross`. For some genres, we only have very limited observations (e.g. thriller, musical, family, romance). Because of this, we are unable to draw any conclusions of whether such correlation exists for these genres. 


# Returns of financial stocks

As we have to pay rather high tuition fees, our idea is to make some money with stocks. We want to download some data for interesting companies that are listed at the NYSE and examine risks and returns!
But in order to download the financial data, we first have to consider the mapping of company names to stock symbols. 


```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
```


Now that we have this data, let's have a quick look at the number of companies per sector.


```{r companies_per_sector}

companies_by_sector <- nyse %>%
  group_by(sector) %>%
  count() %>% # count the number of companies by sector
  arrange(desc(n))

companies_by_sector %>% 
  rename(`number of companies` = n) %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))


ggplot(companies_by_sector, aes(x = n, y = reorder(sector, n)))+
  geom_col() + 
  labs(title = "Number of NYSE companies per sector",
       x = "number of companies",
       y = "sector")

```

Next, let's choose some stocks and their ticker symbols and download some data. Note that `SPY` is the SP500 ETF (Exchange Traded Fund).


```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}

myStocks <- c("AXP","BLK","KO","FDX","MCD","ORCL","SPY" ) %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2020-08-31") %>%
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame

```

Financial performance analysis depends on returns. Hence, we first want to calculate daily and monthly returns (given the adjusted closing prices). 


```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```


To get a feeling for the numbers, let's summarise monthly returns for each of the stocks and the `SPY` ETF:

```{r summarise_monthly_returns}

myStocks_returns_monthly_summary <- myStocks_returns_monthly %>% 
  group_by(symbol) %>% 
  summarise(min = min(monthly_returns), 
            median = median(monthly_returns), 
            max = max(monthly_returns), 
            mean = mean(monthly_returns), 
            sd = sd(monthly_returns)) # calculate summary statistics for monthly returns by stock

myStocks_returns_monthly_summary %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

To get an even better feeling, let's also visualise the distribution of monthly returns for each of the stocks:

```{r density_monthly_returns}

myStocks_returns_monthly %>% 
  filter(symbol != "SPY") %>% # only display density of the stocks (not the ETF)
  ggplot(aes(x = monthly_returns, fill = symbol)) + # add some color (fill) just for fun
  geom_density() + 
  facet_wrap(~symbol) + # create a density plot for each stock
  labs(title = "Distribution of monthly returns per stock", 
       x = "monthly returns") +
  theme(legend.position = "none") # show no legend

```
We can see that deviation from 0.0 is strongest for Blackrock (BLK). Hence, the stock is highly volatile and thus the riskiest.
Opposed to this, the stock of Coca Cola (KO) is quite steady, with only small changes in monthly return. Hence, this stock
is the least risky.

However, it is not only about risk, but also about returns. Hence, let's plot the risk/return profiles of our stocks!

```{r risk_return_plot}

myStocks_returns_monthly_summary %>% 
  filter(symbol != "SPY") %>% # only display density of the stocks
  ggplot(aes(x=sd, y = mean, label = symbol)) +
  geom_point(aes(colour = symbol), size = 4) + # make points a little bigger
  geom_smooth(method = lm, se = FALSE, group = 0) +
  ggrepel::geom_text_repel(aes(colour = symbol)) + # add the stock names as lables to the points
  labs(title = 'Risk/Return profile of stocks', 
       x = 'Risk (stdev of monthly returns)', 
       y ="Average monthly return") +
  theme_bw() + # use a nice theme
  scale_x_continuous(labels = scales::percent) + # display x values as percentages
  scale_y_continuous(labels = scales::percent) + # display y values as percentages
  theme(legend.position = "none") # show no legend

```

As a thumb of rule, taking a higher risk is rewarded with a higher average monthly return. 
We can see that even though FedEx (FDX) has the highest risk, it does not have the highest average monthly return.
Furthermore, Blackrock (BLK) has the highest average monthly return, with only a little more risk than American Express (AXP) and Oracle (ORCL).
Regarding our goal of making money with stocks, we would thus choose a combination of Blackrock and the lower risk McDonald's (MCD), as they are above our risk-return line.


# IBM HR Analytics


In this section, we analyse a data set on Human Resoruce Analytics. The [IBM HR Analytics Employee Attrition & Performance data set](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) is a fictional data set created by IBM data scientists.  Among other things, the data set includes employees' income, their distance from work, their position in the company, their level of education, etc. A full description can be found on the website.

First, let us load the data.

```{r}

hr_dataset <- read_csv(here::here("data", "datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv"))

```

We are going to clean the data set, as variable names are in capital letters, some variables are not really necessary, and some variables, e.g., `education` are given as a number rather than a more useful description.


```{r}

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)

```

## Analysis

Let's start by getting familiar with the data:

```{r hr_glimpse}

glimpse(hr_cleaned)
skim(hr_cleaned)

```

Using the `skim` output, we can see that we are dealing with a nice dataset with no missing values. 

As we are in the shoes of IBM, talent retention is obviously a very relevant topic for us. Hence, let's see how often people leave the
company:

```{r attrition}

ggplot(hr_cleaned, aes(x = attrition)) + 
  geom_bar() +
  labs(title = "The majority of employees stays at IBM.")

```

To give concrete numbers, around 84% of people stay at IBM:

```{r attrition_2}

hr_cleaned %>%
  group_by(attrition) %>%
  summarise(n = n()) %>%
  mutate(proportion = n / sum(n)) %>% # calculate proportion of people who left vs stayed (i.e. by attrition)
  select(attrition, proportion) %>%  # kill counting column (n)
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

However, it's not only about the mere number, but also about which kind of people leave. If only low performers stay with IBM and all high
achievers are leaving, this would be pretty bad. Thus, lets have a look at `attrition` in combination with `performance_rating`:

```{r attrition_performance}

ggplot(hr_cleaned, aes(x = attrition, fill = performance_rating)) + 
  geom_bar(position = "fill") + # show performance rating proportion by attrition (with bars both at 100%)
  labs(fill = "performance rating",
       y = "proportion",
       title = "Attrition and performance rating are independent",
       subtitle = "Performance rating proportion by attrition")

```

We can be relieved: the performance of people has no influence on attrition.

Other important factors for a company are the distributions of `age`, `years_at_company`, `monthly_income` and `years_since_last_promotion`.
Hence, let us have a quick look at some summary statistics and the distributions:

```{r distributions_summary}

hr_cleaned %>% 
  select(age, years_at_company, monthly_income, years_since_last_promotion) %>% # kill useless columns
  summarise_all(list(min, median, max, mean, sd)) %>% # compute summary statistics for each selected variable
  pivot_longer(age_fn1:years_since_last_promotion_fn5, names_to = "names", values_to = "value") %>% # make columns to rows
  separate(names, into = c("variable", "func"), sep = "_fn") %>% # split up "age_fn1" into "age" and "1"
  mutate( # translate back the numbers, that summarise_all has assigned to the function names
    func = case_when(
      func == 1 ~ "min",
      func == 2 ~ "median",
      func == 3 ~ "max",
      func == 4 ~ "mean",
      func == 5 ~ "sd"
    )
  ) %>% 
  arrange(variable) %>% # order by variable name, to see all summary statistics for one variable one below the other
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
  
ggplot(hr_cleaned, aes(x = age)) + 
  geom_density()+
  labs(x = "age",
       title = "Distribution of age")

ggplot(hr_cleaned, aes(x = years_at_company)) + 
  geom_density()+
  labs(x = "years at company",
       title = "Distribution of years at company")

ggplot(hr_cleaned, aes(x = monthly_income)) + 
  geom_density()+
  labs(x = "monthly income",
       title = "Distribution of monthly income")

ggplot(hr_cleaned, aes(x = years_since_last_promotion)) + 
  geom_density() +
  labs(x = "years since last promotion",
       title = "Distribution of years since last promotion")


```

Looking at median and mean, it seems like only age is symmetric and thus roughly Normal.
We can reinforce this finding by looking at the density plots.

We now want to analyse `job_satisfaction` and `work_life_balance`.
First, let's have a look at the mere numbers:

```{r job_sat_wlb}

hr_cleaned %>%
  group_by(job_satisfaction) %>%
  summarise(`number of people` = n()) %>%
  mutate(proportion = `number of people` / sum(`number of people`)) %>% # calculate proportion of people by job satisfaction level
  arrange(desc(factor(job_satisfaction, levels = c("Low", "Medium", "High", "Very High")))) %>% # reorder job satisfaction levels properly
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

ggplot(hr_cleaned, aes(x = factor(job_satisfaction, levels = c("Low", "Medium", "High", "Very High")))) +
  geom_bar(aes(y = stat(prop), group = 1)) + # do not show absolute counts but the proportion in percent (stat(prop))
  scale_y_continuous(labels = scales::percent) + # show y values as percentages
  labs(x = "job satisfaction",
       title = "Proportion of people by job satisfaction")
  
hr_cleaned %>%
  group_by(work_life_balance) %>%
  summarise(`number of people` = n()) %>%
  mutate(proportion = `number of people` / sum(`number of people`)) %>% # calculate proportion of people by work life balance level
  arrange(desc(factor(work_life_balance, levels = c("Bad", "Good", "Better", "Best")))) %>% # reorder work life balance levels properly
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

ggplot(hr_cleaned, aes(x = factor(work_life_balance, levels = c("Bad", "Good", "Better", "Best")))) +
  geom_bar(aes(y = stat(prop), group = 1)) + # do not show absolute counts but the proportion in percent (stat(prop))
  scale_y_continuous(labels = scales::percent) + # show y values as percentages
  labs(x = "work-life-balance",
       title = "Proportion of people by work-life-balance")

```

We can see that most people (over 60%) are highly satisfied with their job. Furthermore, only around 5% of people have a bad work-life-balance.
Is there any relationship between those two variables? Let's have a look:

```{r job_sat_wlb_2}

ggplot(hr_cleaned, aes(x = factor(job_satisfaction, levels = c("Low", "Medium", "High", "Very High")), # order job satisfactioon properly
                       fill = factor(work_life_balance, levels = c("Best", "Better", "Good", "Bad")))) + # order w-l-b properly
  geom_bar(position = "fill") +
  labs(fill = "work life balance",
       y = "proportion",
       title = "Job Satisfaction and Work Life Balance are independent",
       subtitle = "Work life balance proportion by job satisfaction",
       x = "job satisfaction")
```

Hence, it seems like there is no apparent relationship between work-life-balance and job satisfaction!

Other relationships of interest are between monthly income and education and also monthly income and gender.
Let's see what we find out:

```{r relationship_income_edu}

ggplot(hr_cleaned, mapping = aes(x = monthly_income, y = stat(density))) + # show density, not counts (stat(density))
  geom_freqpoly(mapping = aes(colour = education)) +
  labs(x = "monthly income",
       title = "monthly income distribution by education") +
  facet_wrap(~factor(education, levels = c("Below College", "College", "Bachelor", "Master", "Doctor"))) + # order facets properly
  theme_gdocs() # use a nice theme

ggplot(hr_cleaned, aes(y = factor(education, levels = c("Below College", "College", "Bachelor", "Master", "Doctor")), 
                       x = monthly_income)) +
  geom_boxplot() +
  labs(x = "monthly income",
       y = "education",
       title = "monthly income boxplots by education")

```

In the boxplot above, we can see that the median monthly income is higher with higher education, with the exemption of
a lower median monthly income for a Bachelor education as opposed to College education.
Let's complement this analysis with a look at average monthly income by education level:

```{r avg_income_edu}

avg_income_by_edu <- hr_cleaned %>% 
  group_by(education) %>% 
  summarise(mean = mean(monthly_income)) # compute average monthly income by education level

ggplot(avg_income_by_edu, aes(x = factor(education, levels = c("Below College", "College", "Bachelor", "Master", "Doctor")), # order education levels properly
                              y = mean)) +
  geom_col() +
  labs(x = "education level",
       y = "average monthly income",
       title = "Higher education means higher average income!")


```

There is definitely a tendency for higher education yielding higher average monthly incomes!

```{r relationship_income_gender}

ggplot(hr_cleaned, mapping = aes(x = monthly_income, y = stat(density))) + # show density, not counts (stat(density))
  geom_freqpoly(mapping = aes(colour = gender)) +
  labs(x = "monthly income",
       title = "monthly income distribution by gender") +
  facet_wrap(~gender)

ggplot(hr_cleaned, aes(y = gender, 
                       x = monthly_income)) +
  geom_boxplot() +
  labs(x = "monthly income",
       y = "gender",
       title = "monthly income boxplots by gender")

```

Despite many articles about the gender pay gap, it seems like women are (slightly) better off than men at IBM!
Even though this is the overall effect, let's take a look at payment by job role. Are both men and women are paid the same in the same job?

```{r gender_pay_job}

ggplot(hr_cleaned, aes(y = gender, 
                       x = monthly_income)) +
  geom_boxplot() +
  facet_wrap(~job_role) +
  labs(x = "monthly income",
       y = "gender",
       title = "monthly income boxplots by gender")

```

In this case, we actually find some evidence for the gender pay gap! As an example, female research directors seem to be paid less as their male colleagues. 

As indicated in the boxplots above, it seems like some job roles are paid much better than others. Hence, let's have a deeper look at the relationship between job role and income.

```{r job_income}

ggplot(hr_cleaned, aes(y = reorder(job_role, monthly_income), x = monthly_income)) + # order by monthly income
  geom_boxplot() +
  labs(x = "monthly income",
       y = "job role",
       title = "monthly income boxplots by job role")

```

As one would expect, managers get paid the most (on average). Sales representatives are least well paid. What happens when we also bring in age?

```{r age_income_job_role}

ggplot(hr_cleaned, aes(x = age, y = monthly_income)) +
  geom_point(alpha = 0.4) + # set opacity of points
  facet_wrap(~job_role) + # one scatterplot for each job role
  labs(y = "monthly income",
       title = "Small effect of age on monthly income given job role")


```

Even though the effect of age on monthly income is not too apparent when faceting on job role, we also have to take into account that
one usually takes on other roles throughout a career. This is why overally there still is an effect of age on income:

```{r age_income}

ggplot(hr_cleaned, aes(x = age, y = monthly_income)) + 
  geom_point(alpha = 0.4) + # set opacity of points
  geom_smooth() +
  labs(y = "monthly income",
       title = "Tendency to have higher income with higher age")


```


# Challenge 1: Replicating a chart

Original chart:

```{r challenge1, echo=FALSE, out.width="90%"}
knitr::include_graphics(here::here("images", "figure3.jpeg"), error = FALSE)
```

Replication:

```{r, fig.width=7.5, fig.height=7.5}
CDC_data <- read_csv(here::here("data", "CDC_Males.csv"))

CDC_data %>% 
  filter(type == "Firearm") %>% # only firearm related suicides / homicides relevant
  filter(!is.na(gun.house.prev.category)) %>% # filter out missing values
  ggplot(aes(x = adjusted.suicide.White, # raw ggplot
             y = adjusted.homicide.White, 
             fill = gun.house.prev.category, 
             size = average.pop.white,
             label = ST)) +
  geom_point(pch=21, color = "gray20", stroke = 1.03) + # raw scatterplot with black contour of size 1.03 around points
  ggrepel::geom_text_repel(size = 4) + # state labels
  labs(x = "White suicide rate (per 100,000 per year)", # add labels
       y = "White homicide rate (per 100,000 per year)",
       fill = "Gun ownership",
       size = "White population") +
  scale_fill_manual(values=c("#FCEDD0", "#FBC178", "#F87847", "#CA1B18")) + # change fill colors
  scale_size_continuous(range = c(0, 14), breaks = c(500000, 1500000, 3000000, 7000000), labels=c("500k", "1.5m", "3m", "7m")) + # change point size and legend breaks and labels
  scale_x_continuous(minor_breaks = c(5, 10, 15, 20, 25, 30)) + # add minor breaks in x axis ticks
  theme(panel.grid.major = element_line(colour = "gray85", size = 0.3), # change color and size of major grid
        panel.grid.minor = element_line(colour = "gray92"), # change color of minor grid
        legend.text = element_text(size = 9), # change size of legend text
        legend.title = element_text(size = 11), # change size of legend title
        panel.background = element_rect(color = "black", size = 0.5, fill = NA), # add panel frame and change fill color to NA / white
        legend.key = element_rect(fill = "gray100"), # no color field beneath legend ticks
        axis.title = element_text(size = 10)) + # change size of axis title
  guides(fill = guide_legend(override.aes = list(size=5), order = 1)) + # change size of legend points under "gun ownership" and change order of legend components (gun ownership above white population)
  coord_fixed(ratio = 6) + # make panel quadratic
  annotate(geom="text", x=25, y=0.75, label="Spearman's rho: 0.74") # add Spearmans rho annotation
  

```


# Challenge 2: 2016 California Contributors plots

Original Chart:

```{r challenge2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "challenge2.png"), error = FALSE)
```


```{r load_CA_data, warnings= FALSE, message=FALSE}

CA_contributors_2016 <- vroom::vroom(here::here("data","CA_contributors_2016.csv"))
zip_code_database <- read_csv(here::here("data","zip_code_database.csv"), 
                              col_types = cols(zip = col_double()))

```

```{r CA_wrangling}

library(tidytext)

CA_plot_data <- CA_contributors_2016 %>% 
  inner_join(zip_code_database) %>% # join cities (zip code translation)
  select(cand_nm, contb_receipt_amt, primary_city) %>% # drop useless columns
  group_by(cand_nm, primary_city) %>% 
  summarise(sum = sum(contb_receipt_amt)) %>% # sum raised money by candidate and city
  filter(cand_nm == "Clinton, Hillary Rodham" | cand_nm == "Trump, Donald J.") %>% # filter for Clinton and Trump
  group_by(cand_nm) %>% 
  top_n(10, sum) %>% # only take those 10 cities with the highest amount of money raised per candidate
  ungroup() %>% 
  mutate(cand_nm = as.factor(cand_nm),
         primary_city = reorder_within(primary_city, sum, cand_nm)) # order cities by total amount raised per candidate

```

Replication:

```{r CA_visualization, fig.width=9, fig.height=4.5}

# separate x axis ticks for Clinton and Trump
my_breaks <- function(x) { if (max(x) < 600000) c(0, 200000, 400000) else c(0, 4000000, 8000000, 12000000) }

ggplot(CA_plot_data, aes(x = primary_city, y = sum, fill = cand_nm)) + # basic ggplot
  geom_col(show.legend = FALSE) + # add bars but show no legend
  facet_wrap(~cand_nm, scales = "free") + # one bar chart per candidate, where scales are inidivdual for Trump and Clinton
  coord_flip() + # turn chart around
  scale_x_reordered() +
  scale_y_continuous(labels = scales::dollar, breaks = my_breaks) + # have custom ticks and show total amount raised as dollar amount
  labs(subtitle = "Where did candidates raise most money?",
       x = NULL,
       y = "Amount raised") + # add labels
  scale_fill_manual(values=c("#2e75c0", "#cb4549")) + # change fill colors
  theme(panel.grid.major = element_line(colour = "gray91"), # change color and size of major grid
        panel.grid.minor = element_line(colour = "gray91"), # change color of minor grid
        panel.background = element_rect(color = "black", size = 0.5, fill = NA), # add panel frame and change fill color to NA / white
        plot.subtitle = element_text(size = 10), # change subtitle size 
        axis.title = element_text(size = 9), # change size of axis title
        axis.text=element_text(size=7), # change axis label sizes
        strip.background = element_rect(color = "black", size = 0.5), # add border around facet title
        strip.text = element_text(size=8)) # change facet title sizes
  
  
```


# Details

- Who did you collaborate with: Noor Alameri, Brigita Angkasa, Lujia Huang, Martino Armanini, Marco Laube, Deniz Oezdemir
- Approximately how much time did you spend on this problem set: 10h
- What, if anything, gave you the most trouble: The formatting in the two Challenges


# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.


